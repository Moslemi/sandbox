{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skipgrams in Keras\n",
    "\n",
    "- In this lecture, we will implement Skipgrams in `Keras`.\n",
    "\n",
    "#### Loading in and preprocessing data\n",
    "- Load the Alice in Wonderland data in Corpus using Keras utility\n",
    "- `Keras` has some nice text preprocessing features too!\n",
    "- Split the text into sentences.\n",
    "- Use `Keras`' `Tokenizer` to tokenize sentences into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Basics\n",
    "from __future__ import print_function, division\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import SVG\n",
    "%matplotlib inline\n",
    "\n",
    "# nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import download\n",
    "\n",
    "# keras\n",
    "np.random.seed(13)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot \n",
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll use Alice in Wonderland\n",
    "\n",
    "path = get_file('carrol-alice.txt', origin=\"http://www.gutenberg.org/files/11/11-0.txt\")\n",
    "corpus = open(path).read()\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093 1093\n"
     ]
    }
   ],
   "source": [
    "# Split document into sentences first\n",
    "corpus = corpus[corpus.index('\\n\\n')+2:]  # remove header.\n",
    "sentences = sent_tokenize(corpus)\n",
    "\n",
    "# Tokenize using Keras\n",
    "base_filter='!\"#$%&()*+,-./:;`<=>?@[\\\\]^_{|}~\\t\\n' + \"'\"\n",
    "tokenizer = Tokenizer(filters=base_filter)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Convert tokenized sentences to sequence format\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "\n",
    "print(len(sequences), tokenizer.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Caterpillar was the first to speak.\n",
      "[1, 182, 13, 1, 98, 4, 330]\n",
      "[1, 182, 13, 1, 98, 4, 330]\n"
     ]
    }
   ],
   "source": [
    "# To understand what is happening;\n",
    "\n",
    "print(sentences[324])  # this is a sentence\n",
    "print(sequences[324])  # this is the same sentence where words are encoded as numbers.\n",
    "print(list(tokenizer.word_index[word.lower().replace('.', '')] \n",
    "           for word in sentences[324].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Skipgrams: Generating Input and Output Labels\n",
    "- Now that we have sentences, and word tokenization, we are in good position to create our training set for skipgrams.\n",
    "- Now we need to generate our `X_train` and `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was first\n",
      "was caterpillar\n",
      "was the\n",
      "was the\n"
     ]
    }
   ],
   "source": [
    "# Let's first see how Keras' skipgrams function works.\n",
    "\n",
    "couples, labels = skipgrams(sequences[324], len(tokenizer.word_index) + 1,\n",
    "    window_size=2, negative_samples=0, shuffle=True,\n",
    "    categorical=False, sampling_table=None)\n",
    "\n",
    "index_2_word = {val: key for key, val in tokenizer.word_index.items()}\n",
    "\n",
    "for w1, w2 in couples:\n",
    "    if w1 == 13:\n",
    "        print(index_2_word[w1], index_2_word[w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function to generate the inputs and outputs for all windows\n",
    "\n",
    "# Vocab size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# Dimension to reduce to\n",
    "dim = 100\n",
    "window_size = 2\n",
    "\n",
    "\n",
    "def generate_data(sequences, window_size, vocab_size):\n",
    "    for seq in sequences:\n",
    "        X, y = [], []\n",
    "        couples, _ = skipgrams(\n",
    "            seq, vocab_size,\n",
    "            window_size=window_size, negative_samples=0, shuffle=True,\n",
    "            categorical=False, sampling_table=None)\n",
    "        if not couples:\n",
    "            continue\n",
    "        for in_word, out_word in couples:\n",
    "            X.append(in_word)\n",
    "            y.append(np_utils.to_categorical(out_word, vocab_size))\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        X = X.reshape(len(X), 1)\n",
    "        y = y.reshape(len(X), vocab_size)\n",
    "        yield X, y\n",
    "        \n",
    "data_generator = generate_data(sequences, window_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skipgrams: Creating the Model\n",
    "- Lastly, we create the (shallow) network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"294pt\" viewBox=\"0.00 0.00 398.00 294.00\" width=\"398pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 290)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-290 394,-290 394,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139818387497816 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139818387497816</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 390,-212.5 390,-166.5 0,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.5\" y=\"-185.8\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"203,-166.5 203,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"237\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"203,-189.5 271,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"237\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"271,-166.5 271,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.5\" y=\"-197.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"271,-189.5 390,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.5\" y=\"-174.3\">(None, 1, 100)</text>\n",
       "</g>\n",
       "<!-- 139818387499552 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139818387499552</title>\n",
       "<polygon fill=\"none\" points=\"21.5,-83.5 21.5,-129.5 368.5,-129.5 368.5,-83.5 21.5,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.5\" y=\"-102.8\">reshape_1: Reshape</text>\n",
       "<polyline fill=\"none\" points=\"181.5,-83.5 181.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"215.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"181.5,-106.5 249.5,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"215.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"249.5,-83.5 249.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-114.3\">(None, 1, 100)</text>\n",
       "<polyline fill=\"none\" points=\"249.5,-106.5 368.5,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-91.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 139818387497816&#45;&gt;139818387499552 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139818387497816-&gt;139818387499552</title>\n",
       "<path d=\"M195,-166.3799C195,-158.1745 195,-148.7679 195,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"198.5001,-139.784 195,-129.784 191.5001,-139.784 198.5001,-139.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139821256710856 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139821256710856</title>\n",
       "<polygon fill=\"none\" points=\"42,-.5 42,-46.5 348,-46.5 348,-.5 42,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106\" y=\"-19.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"170,-.5 170,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"204\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"170,-23.5 238,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"204\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"238,-.5 238,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293\" y=\"-31.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"238,-23.5 348,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293\" y=\"-8.3\">(None, 3399)</text>\n",
       "</g>\n",
       "<!-- 139818387499552&#45;&gt;139821256710856 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139818387499552-&gt;139821256710856</title>\n",
       "<path d=\"M195,-83.3799C195,-75.1745 195,-65.7679 195,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"198.5001,-56.784 195,-46.784 191.5001,-56.784 198.5001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139818387500840 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139818387500840</title>\n",
       "<polygon fill=\"none\" points=\"119,-249.5 119,-285.5 271,-285.5 271,-249.5 119,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"195\" y=\"-263.8\">139818387500840</text>\n",
       "</g>\n",
       "<!-- 139818387500840&#45;&gt;139818387497816 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139818387500840-&gt;139818387497816</title>\n",
       "<path d=\"M195,-249.4092C195,-241.4308 195,-231.795 195,-222.606\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"198.5001,-222.5333 195,-212.5333 191.5001,-222.5334 198.5001,-222.5333\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Keras model and view it \n",
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=vocab_size, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram.add(Reshape((dim,)))\n",
    "skipgram.add(Dense(input_dim=dim, units=vocab_size, activation='softmax'))\n",
    "SVG(model_to_dot(skipgram, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skipgrams: Compiling and Training\n",
    "- Time to compile and train\n",
    "- We use crossentropy, common loss for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "iteration 0, loss is 8478.368892669678\n",
      "iteration 1, loss is 7775.793533325195\n",
      "iteration 2, loss is 7389.948025226593\n",
      "iteration 3, loss is 7170.008626937866\n",
      "iteration 4, loss is 7037.915714979172\n",
      "iteration 5, loss is 6950.122810125351\n",
      "iteration 6, loss is 6886.1554362773895\n",
      "iteration 7, loss is 6835.780106306076\n",
      "iteration 8, loss is 6793.286994934082\n",
      "iteration 9, loss is 6755.320260286331\n"
     ]
    }
   ],
   "source": [
    "# Compile the Keras Model\n",
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr=1e-4, decay=1e-6, momentum=0.9)\n",
    "\n",
    "skipgram.compile(loss='categorical_crossentropy', optimizer=\"adadelta\")\n",
    "\n",
    "# Fit the Skipgrams\n",
    "for iteration in range(10):\n",
    "    loss = 0\n",
    "    for x, y in generate_data(sequences, window_size, vocab_size):\n",
    "        loss += skipgram.train_on_batch(x, y)\n",
    "    print('iteration {}, loss is {}'.format(iteration, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgrams: Looking at the vectors\n",
    "\n",
    "To get word_vectors now, we look at the weights of the first layer.\n",
    "\n",
    "Let's also write functions giving us similarity of two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9519008994102478\n",
      "\n",
      "gryphon     0.969170\n",
      "dormouse    0.966100\n",
      "duchess     0.965377\n",
      "march       0.957099\n",
      "hatter      0.956556\n",
      "king        0.951901\n",
      "cat         0.945495\n",
      "white       0.943479\n",
      "mouse       0.939789\n",
      "rabbit      0.938334\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "word_vectors = skipgram.get_weights()[0]\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def get_dist(w1, w2):\n",
    "    i1, i2 = tokenizer.word_index[w1], tokenizer.word_index[w2]\n",
    "    v1, v2 = word_vectors[i1], word_vectors[i2]\n",
    "    return cosine(v1, v2)\n",
    "\n",
    "def get_similarity(w1, w2):\n",
    "    return 1-get_dist(w1, w2)\n",
    "\n",
    "def get_most_similar(w1, n=10):\n",
    "    sims = {word: get_similarity(w1, word) \n",
    "            for word in tokenizer.word_index.keys()\n",
    "            if word != w1}\n",
    "    sims = pd.Series(sims)\n",
    "    sims.sort_values(inplace=True, ascending=False)\n",
    "    return sims.iloc[:n]\n",
    "\n",
    "\n",
    "print(get_similarity('king', 'queen'))\n",
    "print('')\n",
    "print(get_most_similar('queen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Your turn -- Modify the code above to create a CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss is 8483.155016422272\n",
      "iteration 1, loss is 7773.969968318939\n",
      "iteration 2, loss is 7389.700431346893\n",
      "iteration 3, loss is 7175.441267490387\n",
      "iteration 4, loss is 7047.570253372192\n",
      "iteration 5, loss is 6962.255543231964\n",
      "iteration 6, loss is 6899.337002038956\n",
      "iteration 7, loss is 6848.798505067825\n",
      "iteration 8, loss is 6805.351869344711\n",
      "iteration 9, loss is 6766.067973613739\n"
     ]
    }
   ],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "cbow.add(Reshape((dim,)))\n",
    "cbow.add(Dense(input_dim=dim, units=vocab_size, activation='softmax'))\n",
    "\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "for iteration in range(10):\n",
    "    loss = 0\n",
    "    for x, y in generate_data(sequences, window_size, vocab_size):\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "    print('iteration {}, loss is {}'.format(iteration, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9534236788749695\n",
      "\n",
      "gryphon        0.974086\n",
      "dormouse       0.964757\n",
      "hatter         0.960941\n",
      "duchess        0.956020\n",
      "king           0.953424\n",
      "march          0.947670\n",
      "white          0.945183\n",
      "caterpillar    0.944133\n",
      "cat            0.944020\n",
      "mouse          0.943159\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "word_vectors = cbow.get_weights()[0]\n",
    "print(get_similarity('king', 'queen'))\n",
    "print('')\n",
    "print(get_most_similar('queen'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "livereveal": {
   "height": "100%",
   "margin": 0,
   "maxScale": 1,
   "minScale": 1,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky",
   "transition": "zoom",
   "width": "100%"
  },
  "toc": {
   "nav_menu": {
    "height": "369px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "457px",
    "left": "0px",
    "right": "968px",
    "top": "130px",
    "width": "214px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
