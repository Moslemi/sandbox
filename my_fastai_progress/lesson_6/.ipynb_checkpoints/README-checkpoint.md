# My progress in fastai course - Lesson 6

#### Useful links:

1. [Video](https://course.fast.ai/videos/?lesson=6)
2. [Documentation](https://docs.fast.ai)
3. [Lesson discussion thread](https://forums.fast.ai/t/lesson-6-in-class-discussion/31440)
4. [hiromis (GitHub)](https://github.com/hiromis/notes/blob/master/Lesson6.md)
5. [Lesson 6 Advanced Discussion](https://forums.fast.ai/t/lesson-6-advanced-discussion/31442)


#### Data set, which I used:

   * [Rossman dataset (Kaggle competition)](files.fast.ai/part2/lesson14/rossmann.tgz);

   Problem:  regularization and convolutions.

#### My notes:

   - **data augumentation** - do
   - **regularization**:
       * dropout,
       * batch normalization;
       
       ![...](file/name)
       
#### Tips:

   1. **...** - ... 

#### Other resources:

   * [CS1114 Section 6: Convolution](http://www.cs.cornell.edu/courses/cs1114/2013sp/sections/S06_convolution.pdf);
   * [Convolution arithmetic](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md);
   * [Normalization in Deep Learning](https://arthurdouillard.com/post/normalization/);
   * [Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names](https://gombru.github.io/2018/05/23/cross_entropy_loss/);
   * [How do Convolutional Neural Networks work?](https://brohrer.github.io/how_convolutional_neural_networks_work.html);
   * [Image Processing and Computer Vision](https://openframeworks.cc/ofBook/chapters/image_processing_computer_vision.html);
   * [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270);
   * [CCNBook/Perception](https://grey.colorado.edu/CompCogNeuro/index.php/CCNBook/Perception);
   